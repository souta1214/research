{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">ライブラリのインストール</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jupyter-contrib-nbextensions\n",
    "# !pip install jupyter-nbextensions-configurator\n",
    "\n",
    "# !jupyter contrib nbextension install\n",
    "# !jupyter nbextensions_configurator enable\n",
    "# !pip install torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">cpuとgpuの選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">乱数を特定の値にして再現性を確保"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"5\">pytorch用データセットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "データサイズの合計が元のデータセットのサイズと一致しません。データサイズを調整してください。\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "# pytorchではTensorというデータ構造で、モデルの入力、出力、パラメータを表現\n",
    "# 前処理の定義\n",
    "\"\"\"\n",
    "これから取得するDatasetの中身がndarray型のデータ集合であるため、前処理でtensor型にしたい\n",
    "\n",
    "root\n",
    "Datasetを参照(または保存)するディレクトリを「path」の部分に指定する.\n",
    "そのディレクトリに取得したいDatasetが存在すればダウンロードせずにそれを使用する.\n",
    "\n",
    "train\n",
    "Training用のdataを取得するかどうかを選択する.\n",
    "FalseにすればTest用のdataを取得するが,この2つの違いはdata数の違いと思ってくれて良い.\n",
    "\n",
    "download\n",
    "rootで参照したディレクトリにDatasetがない場合ダウンロードするかどうかを決めることができる.\n",
    "\n",
    "transform\n",
    "定義した前処理を渡す.\n",
    "こうすることでDataset内のdataを参照する時にその前処理を自動で行ってくれる.\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# FashionMNISTデータセットの読み込みと前処理\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# 元のデータセットの読み込み\n",
    "original_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# データを訓練、検証、テストに分割\n",
    "total_size = len(original_dataset)\n",
    "print(total_size)\n",
    "train_size = 30000\n",
    "valid_size = 30000\n",
    "test_size = 1000\n",
    "\n",
    "# データサイズの合計が元のデータセットのサイズと一致していることを確認\n",
    "if train_size + valid_size + test_size == total_size:\n",
    "    train_dataset, valid_dataset, test_dataset = random_split(original_dataset, [train_size, valid_size, test_size])\n",
    "    print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
    "    print(f\"Validation Dataset Size: {len(valid_dataset)}\")\n",
    "    print(f\"Test Dataset Size: {len(test_dataset)}\")\n",
    "else:\n",
    "    print(\"データサイズの合計が元のデータセットのサイズと一致しません。データサイズを調整してください。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset_original' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-672d5ed4f7ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# クラス数を取得\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset_original\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset_original\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset_original' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "# # データセットをロード\n",
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "# dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# クラス数を取得\n",
    "num_classes = len(train_dataset_original.classes)\n",
    "print(num_classes)\n",
    "print(len(train_dataset_original))\n",
    "# 各クラスごとのデータ数を計算\n",
    "desired_class_count = 3000  # 各クラスごとに均等に分割したいデータ数\n",
    "total_desired_count = desired_class_count * num_classes\n",
    "\n",
    "# クラスごとに均等にデータを分割するためのリストを作成\n",
    "selected_indices_1 = []\n",
    "selected_indices_2 = []\n",
    "\n",
    "# 各クラスごとに均等にデータを選択\n",
    "for class_idx in range(num_classes):\n",
    "    class_indices = np.where(train_dataset_original.targets == class_idx)[0]\n",
    "    np.random.shuffle(class_indices)\n",
    "    selected_indices_1.extend(class_indices[:desired_class_count])\n",
    "    selected_indices_2.extend(class_indices[desired_class_count:2*desired_class_count])\n",
    "\n",
    "# 選択したデータで新しいデータセットを作成\n",
    "subset_dataset_1 = torch.utils.data.Subset(train_dataset_original, selected_indices_1)\n",
    "subset_dataset_2 = torch.utils.data.Subset(train_dataset_original, selected_indices_2)\n",
    "\n",
    "# # 各クラスごとの新しいデータ数を確認\n",
    "class_counts_subset_1 = np.bincount([train_dataset_original.targets[i] for i in selected_indices_1])\n",
    "class_counts_subset_2 = np.bincount([train_dataset_original.targets[i] for i in selected_indices_2])\n",
    "\n",
    "print(\"データセット1の各クラスごとの新しいデータ数:\", class_counts_subset_1)\n",
    "print(\"データセット2の各クラスごとの新しいデータ数:\", class_counts_subset_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"5\">Siamese Network用MNISTデータセットクラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese Network用MNISTデータセットクラスの作成\n",
    "class SiameseMNIST(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset=dataset\n",
    "        self.length=len(self.dataset)\n",
    "        self.pair_index=[]     # Siamese Net用画像ペアインデックス配列\n",
    "        labels=[label for _, label in self.dataset]     # 入力されたデータセットからラベル情報のみ抽出\n",
    "        positive_count=0     # positiveペアのカウント\n",
    "        negative_count=0     # negativeペアのカウント\n",
    "        random_index=np.arange(self.length)     # ？？？\n",
    "        \n",
    "        while positive_count + negative_count < self.length:\n",
    "            np.random.shuffle(random_index)\n",
    "            for i in np.arange(self.length):\n",
    "                if labels[i]==labels[random_index[i]]:     # 画像ペアのラベルが等しい時（positive）\n",
    "                    if positive_count<self.length/2:      # おそらく比率を同じにしたい？？\n",
    "                        self.pair_index.append([i, random_index[i], 1])      # 要素の構成：[<画像1のインデックス>,<画像2のインデックス>,<posi:1/nega:0フラグ>]\n",
    "                        positive_count+=1\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    if negative_count<self.length/2:\n",
    "                        self.pair_index.append([i, random_index[i], 0])\n",
    "                        negative_count+=1\n",
    "                    else:\n",
    "                        continue\n",
    "#         print(\"com\", self.pair_index)\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.dataset[self.pair_index[index][0]][0], self.dataset[self.pair_index[index][1]][0], torch.tensor(self.pair_index[index][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"5\">Siamese Network学習用Dataset, DataLoaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "train_dataset=SiameseMNIST(subset_dataset_1) \n",
    "train_loader_learn=DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_loader_projection=DataLoader(subset_dataset_2, batch_size=batch_size, shuffle=True)\n",
    "test_loader=DataLoader(test_dataset_original, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"5\">ペア画像の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, X2, y = next(iter(train_loader_learn))\n",
    "print(X1[0].shape)\n",
    "fig=plt.figure(tight_layout=True, figsize=(8, 16))\n",
    "rows=10\n",
    "for i in range(rows):\n",
    "    print(f\"y[{i}]={y[i]}\")\n",
    "    ax = fig.add_subplot(rows, 2, i*2+1)\n",
    "    ax.imshow(X1[i][0].numpy(), cmap='gray')                          # X1[i].shape = (1, 28, 28)，X1[i][0].shape = (28, 28)\n",
    "    ax = fig.add_subplot(rows, 2, i*2+2)\n",
    "    ax.imshow(X2[i][0].numpy(), cmap='gray') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">モデルの構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "super()はあるクラス（子クラス）で別のクラス（親クラス）を継承できる。継承することで、親クラスのメソッドを子クラスから呼び出すことができる。\n",
    "以下の形式\n",
    "super().親クラスのメソッド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Base network for Siamese Network.\n",
    "    This will be shared between two input images.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the layers\n",
    "        # Using similar architecture as in the Keras example\n",
    "\n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Second Convolutional Layer\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Flatten and Dense Layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Assuming input image size is 28x28\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "    def forward_1(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "#         print(\"xのshape\",x)\n",
    "        x = self.act4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1,x2):\n",
    "        z1=self.forward_1(x1)\n",
    "        z2=self.forward_1(x2)\n",
    "        return z1,z2\n",
    "\n",
    "# Test the base network with a sample input\n",
    "base_network = BaseNetwork().to(device)\n",
    "sample_input = torch.randn(8, 1, 28, 28).to(device)  # Batch of 8 images of sizammm28x28 with 1 channel\n",
    "sample_output = base_network.forward_1(sample_input)\n",
    "\n",
    "sample_output.shape  # Expected: [8, 256]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamse Networkモデルクラス\n",
    "import torch.nn as nn\n",
    "\n",
    "class SiameseCNNMnistModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, padding=2), # 28x28x32\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),                         # 14x14x32\n",
    "            nn.Conv2d(32, 64, kernel_size=5, padding=2),# 14x14x64\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)                          # 7x7x64\n",
    "        )\n",
    "\n",
    "        # Fully connected layers to get embeddings\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(7*7*64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "    \n",
    "    def forward_once(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size()[0], -1)  # Flatten\n",
    "        z = self.fc(x)\n",
    "        return z\n",
    "  \n",
    "    def forward(self, x1, x2):\n",
    "        z1 = self.forward_once(x1)\n",
    "        z2 = self.forward_once(x2)\n",
    "        return z1, z2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">損失関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.5):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, z1, z2, y):\n",
    "        difference = z1 - z2\n",
    "        distance_squared = torch.sum(torch.pow(difference, 2), 1)\n",
    "        distance = torch.sqrt(distance_squared)       #平均：0.813，最大：1.663，最小：0.023，中央値：0.492\n",
    "        negative_distance = self.margin - distance\n",
    "        negative_distance = torch.clamp(negative_distance, min=0.0)\n",
    "        loss = (abs(y-0.2) * distance_squared + (1 - (y-0.2)) * torch.pow(negative_distance, 2)) / 2.0\n",
    "        loss = torch.sum(loss) / z1.size()[0]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"5\">学習の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "#モデルのインスタンス化\n",
    "model=SiameseCNNMnistModel().to(device)\n",
    "print(model.parameters)\n",
    "# ？？？？\n",
    "summary(model, input_size=[(1, 28, 28), (1, 28, 28)])      # 入力が２つあるので（ペア画像だから）input_sizeはリストで複数指定する\n",
    "\n",
    "# 最適化関数の定義\n",
    "optimizer=optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "# 損失関数のインスタンス化\n",
    "criterion=ContrastiveLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"5\">学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# モデル学習\n",
    "repeat = 1                                                      # 学習回数\n",
    "losses = []                                                       # 表示用損失値配列\n",
    "\n",
    "model.train()                                                     # 学習モード\n",
    "for epoch in range(repeat): \n",
    "  print(f\"epoch={epoch}\")\n",
    "  nan_count = 0\n",
    "  normal_count = 0\n",
    "\n",
    "  # tqdmを使って学習の進行状況を表示\n",
    "  for X1, X2, y in tqdm(train_loader_learn, desc=f\"Epoch {epoch+1}/{repeat}\"):                                  \n",
    "    # モデルによる特徴ベクトル算出\n",
    "    output1, output2 = model(X1.to(device), X2.to(device))\n",
    "\n",
    "    # 損失関数の計算\n",
    "    loss = criterion(output1, output2, y.to(device))\n",
    "\n",
    "    # nan対策（lossにnanが含まれていれば１回前のモデルに戻す）\n",
    "    if torch.isnan(loss):\n",
    "      model = prev_model\n",
    "      optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "      optimizer.load_state_dict(prev_optimizer.state_dict())\n",
    "      nan_count += 1\n",
    "      continue\n",
    "    else:\n",
    "      prev_model = copy.deepcopy(model)\n",
    "      prev_optimizer = copy.deepcopy(optimizer)\n",
    "      normal_count += 1\n",
    "\n",
    "    # 表示用lossデータの記録\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # 勾配を初期化\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 損失関数の値から勾配を求め誤差逆伝播による学習実行\n",
    "    loss.backward()\n",
    "    \n",
    "    # 学習結果に基づきパラメータを更新\n",
    "    optimizer.step()\n",
    "\n",
    "  print(f\"nan/normal: {nan_count}/{normal_count}\")\n",
    "plt.plot(losses)                                                  # loss値の推移を表示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習結果の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習済みの重みを再利用する場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('model_weights.pth''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =\"5\">モデルの出力ベクトルの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# サンプルのクラスごとの正解数と不正解数\n",
    "class_correct = [97, 85, 92, 93, 98, 94, 88, 91, 89, 90]  # サンプルの正解数\n",
    "class_total = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]  # サンプルの総数\n",
    "\n",
    "# 各クラスの名前（FashionMNISTの実際のクラス名と対応するものとして仮定）\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# データを整形\n",
    "data=[\n",
    "    class_correct,\n",
    "    class_total\n",
    "]\n",
    "\n",
    "# データフレームを作成\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 行と列の表示\n",
    "df.columns=class_names\n",
    "df.index=['正解数', '不正解数']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル学習の結果、同じクラスに属する画像の出力ベクトルは近くに、異なる数字に属する画像の出力ベクトルは遠くに配置されているはず。\n",
    "これを確認するために、テストデータを用いて出力ベクトルを集める。\n",
    "model.eval()で評価モードに設定し、変数の勾配も再計算されないようにtorch.no_grid()を記述する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル評価\n",
    "model.eval()                                                      # 評価モード\n",
    "with torch.no_grad():\n",
    "    z_test = []\n",
    "    y_test = []\n",
    "    for X, y in test_loader:                                      # テスト用DataLoader\n",
    "        z_test.append(model.forward_once(X.to(device)))           # テストデータをモデルに通して出力ベクトルを得る\n",
    "        y_test.append(y)                                          # ラベル（靴、シャツ）の代わりにラベルIDとして0～9の数代わり振られている\n",
    "    z_test = torch.cat(z_test, dim=0)                             # 多次元torch.tensor要素のリストをtorch.tensor化\n",
    "    y_test = torch.tensor(y_test)                                 # スカラ要素(int)リストをtorch.tensor化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5ebe68409ce4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 次元数の確認\n",
    "print(z_test.size())\n",
    "print(y_test.size())\n",
    "# print(z_test_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z_testとy_testをGPUに移動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUが利用可能かチェック\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# z_testとy_testをGPUに移動\n",
    "z_test = z_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "# ndarrayに変換\n",
    "z_test_np = z_test.to('cpu').detach().numpy().copy()\n",
    "y_test_np = y_test.to('cpu').detach().numpy().copy()\n",
    "# print(z_test_np.shape)\n",
    "# print(y_test_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">モデル出力ベクトルの分布可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# クラス名\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "# クラス数\n",
    "class_num=len(class_names)\n",
    "# 事前に定義された10色のリスト\n",
    "colors = [\"blue\",\"green\",\"red\",\"cyan\",\"magenta\",\"yellow\",\"black\",'#f781bf','#a65628','#ff7f00']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "②　重心を算出（ノイズクラスタリングなし）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重心を算出\n",
    "def caluc_cluster(data, labels, i, class_num):\n",
    "    x_1 = np.mean(data[labels==i, 0], axis=0)\n",
    "    y_1 = np.mean(data[labels==i, 1], axis=0)\n",
    "    z_1 = np.mean(data[labels==i, 2], axis=0)\n",
    "    centroids=(x_1, y_1, z_1)\n",
    "#     print(\"ラベル\", i, \"の重心：\", x_1, y_1, z_1)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "③　単位球面上に射影(目的クラスの重心を原点へ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単位球面上に写像する関数\n",
    "def norm(data):\n",
    "    norm_data=data.copy()\n",
    "    for i in range(len(data)):\n",
    "        norm_data[i]=data[i] / np.linalg.norm(data[i], ord=2)\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "④　データの回転"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def caluc_rotation_main(data):\n",
    "    # 値の入力\n",
    "    x=data[0, 0]\n",
    "    y=data[0, 1]\n",
    "    z=data[0, 2]\n",
    "    \n",
    "    # xの回転\n",
    "    theta_1 = np.arctan2(y, z)\n",
    "    \n",
    "    x_rot_0 = x\n",
    "    x_rot_1 = y*np.cos(theta_1) - z*np.sin(theta_1)\n",
    "    x_rot_2 = y*np.sin(theta_1)+z*np.cos(theta_1)\n",
    "    \n",
    "    # 値の更新\n",
    "    x=x_rot_0\n",
    "    y=x_rot_1\n",
    "    z=x_rot_2\n",
    "    \n",
    "    # yの回転\n",
    "    theta_2 = np.arctan2(-x, z)\n",
    "    \n",
    "    y_rot_0 = x*np.cos(theta_2) + z*np.sin(theta_2)\n",
    "    y_rot_1 = y\n",
    "    y_rot_2 = -x*np.sin(theta_2) + z*np.cos(theta_2)\n",
    "    \n",
    "    ans=(y_rot_0, y_rot_1, y_rot_2)\n",
    "    theta=theta_1, theta_2\n",
    "    \n",
    "    return ans, theta\n",
    "\n",
    "def caluc_rotation_sub(data, theta):\n",
    "    ans=np.zeros((9,3))\n",
    "    for i in range(1,data.shape[0]):\n",
    "        # 値の入力\n",
    "        x=data[i, 0]\n",
    "        y=data[i, 1]\n",
    "        z=data[i, 2]\n",
    "\n",
    "        # xの回転\n",
    "        theta_1 = theta[0]\n",
    "\n",
    "        x_rot_0 = x\n",
    "        x_rot_1 = y*np.cos(theta_1) - z*np.sin(theta_1)\n",
    "        x_rot_2 = y*np.sin(theta_1)+z*np.cos(theta_1)\n",
    "\n",
    "        # 値の更新\n",
    "        x=x_rot_0\n",
    "        y=x_rot_1\n",
    "        z=x_rot_2\n",
    "\n",
    "        # yの回転\n",
    "        theta_2 = theta[1]\n",
    "\n",
    "        y_rot_0 = x*np.cos(theta_2) + z*np.sin(theta_2)\n",
    "        y_rot_1 = y\n",
    "        y_rot_2 = -x*np.sin(theta_2) + z*np.cos(theta_2)\n",
    "        print(i)\n",
    "        ans[i-1]=(y_rot_0, y_rot_1, y_rot_2)\n",
    "    \n",
    "    return ans\n",
    "\n",
    "def caluc_rotation_data(data, theta):\n",
    "    ans=np.zeros((data.shape[0],data.shape[1]))\n",
    "#     print(ans.shape)\n",
    "    for i in range(data.shape[1]):\n",
    "        # 値の入力\n",
    "        x=data[:, 0]\n",
    "        y=data[:, 1]\n",
    "        z=data[:, 2]\n",
    "\n",
    "        # xの回転\n",
    "        theta_1 = theta[0]\n",
    "\n",
    "        x_rot_0 = x\n",
    "        x_rot_1 = y*np.cos(theta_1) - z*np.sin(theta_1)\n",
    "        x_rot_2 = y*np.sin(theta_1)+z*np.cos(theta_1)\n",
    "\n",
    "        # 値の更新\n",
    "        x=x_rot_0\n",
    "        y=x_rot_1\n",
    "        z=x_rot_2\n",
    "\n",
    "        # yの回転\n",
    "        theta_2 = theta[1]\n",
    "\n",
    "        y_rot_0 = x*np.cos(theta_2) + z*np.sin(theta_2)\n",
    "        y_rot_1 = y\n",
    "        y_rot_2 = -x*np.sin(theta_2) + z*np.cos(theta_2)\n",
    "        ans=(y_rot_0, y_rot_1, y_rot_2)\n",
    "        \n",
    "    \n",
    "    return ans\n",
    "\n",
    "# norm_centroids.shape[0]\n",
    "ans, theta = caluc_rotation_main(norm_centroids)\n",
    "print(\"回転行列 ans:\")\n",
    "print(ans)\n",
    "print(theta)\n",
    "ans_sub = caluc_rotation_sub(norm_centroids, theta)\n",
    "print(ans_sub)\n",
    "# ansを合体させる\n",
    "ans_new=np.zeros((10, 3))\n",
    "ans_new[0]=ans\n",
    "ans_new[1:]=ans_sub\n",
    "ans_new\n",
    "new_data=caluc_rotation_data(norm_data, theta)\n",
    "my_array = np.array(new_data, dtype=float)\n",
    "my_array=np.transpose(my_array)\n",
    "my_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# 球体のパラメータ\n",
    "radius = 1\n",
    "phi = np.linspace(0, np.pi, 15)  # 緯度角の範囲\n",
    "theta = np.linspace(0, 2 * np.pi, 30)  # 経度角の範囲\n",
    "phi, theta=np.meshgrid(phi, theta)\n",
    "\n",
    "# 球体の座標を計算\n",
    "x = radius * np.sin(phi) * np.cos(theta)\n",
    "y = radius * np.sin(phi) * np.sin(theta)\n",
    "z = radius * np.cos(phi)\n",
    "\n",
    "\n",
    "# figureを生成する\n",
    "fig=plt.figure(figsize=(10, 10))\n",
    "# axをfigureに設定する\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# 球体をプロット\n",
    "ax.plot_surface(x, y, z, linewidth=1, edgecolor=(0.8, 0.8, 0.8, 1.0), alpha=0)\n",
    "\n",
    "\n",
    "# クラスごとに色を分けてプロット\n",
    "for i in range(class_num):\n",
    "    centroids[i]=caluc_cluster(z_test_np, y_test_np, i, class_num)\n",
    "    norm_data=norm(z_test_np)\n",
    "    norm_centroids=norm(centroids)\n",
    "#     ax.scatter(norm_centroids[i, 0], norm_centroids[i, 1], norm_centroids[i, 2], marker='*', color=colors[i], s=500, linewidths=\"1\", edgecolors=\"black\")\n",
    "    ax.scatter(my_array[y_test_np==i, 0], my_array[y_test_np==i, 1], my_array[y_test_np==i, 2], color=colors[i], label=class_names[i])\n",
    "    ax.scatter(ans_new[i, 0], ans_new[i, 1], ans_new[i, 2], marker='*', color=colors[i], s=500, linewidths=\"1\", edgecolors=\"black\")\n",
    "#     ax.scatter(ans_sub[0], ans_sub[1], ans_sub[2], marker='*', color=colors[1], s=500, linewidths=\"1\", edgecolors=\"black\")\n",
    "ax.legend()  # 凡例の追加\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⑤　二次元空間にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "\n",
    "# 球体のパラメータ\n",
    "radius = 1\n",
    "phi = np.linspace(0, np.pi, 15)  # 緯度角の範囲\n",
    "theta = np.linspace(0, 2 * np.pi, 30)  # 経度角の範囲\n",
    "phi, theta=np.meshgrid(phi, theta)\n",
    "\n",
    "# 球体の座標を計算\n",
    "x = radius * np.sin(phi) * np.cos(theta)\n",
    "y = radius * np.sin(phi) * np.sin(theta)\n",
    "z = radius * np.cos(phi)\n",
    "\n",
    "\n",
    "# figureを生成する\n",
    "fig=plt.figure(figsize=(10, 10))\n",
    "# axをfigureに設定する\n",
    "ax = fig.add_subplot()\n",
    "draw_circle_05 = plt.Circle((0, 0), 0.5, fill=False)\n",
    "draw_circle_1 = plt.Circle((0, 0), 1, fill=False)\n",
    "\n",
    "ax.set_aspect(1)\n",
    "ax.add_artist(draw_circle_05)\n",
    "ax.add_artist(draw_circle_1)\n",
    "\n",
    "\n",
    "\n",
    "# クラスごとに色を分けてプロット\n",
    "for i in range(class_num):\n",
    "    centroids[i]=caluc_cluster(z_test_np, y_test_np, i, class_num)\n",
    "    norm_data=norm(z_test_np)\n",
    "    norm_centroids=norm(centroids)\n",
    "    ax.scatter(my_array[y_test_np==i, 0], my_array[y_test_np==i, 1], color=colors[i], label=class_names[i])\n",
    "    ax.scatter(ans_new[i, 0], ans_new[i, 1], marker='*', color=colors[i], s=500, linewidths=\"1\", edgecolors=\"black\")\n",
    "ax.legend()  # 凡例の追加\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
